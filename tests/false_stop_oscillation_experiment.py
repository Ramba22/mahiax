#!/usr/bin/env python3
"""
False-Stop / Oscillation Robustness Experiment for MAHIA-V5
"""

import sys
import os
import random
import numpy as np
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

# Import the classes we want to test
from modell_V5_MAHIA_HyenaMoE import ExtendStop, GradientEntropyMonitor, AutoLrPrecisionTuner

class RobustnessExperiment:
    """Test false-stop and oscillation robustness"""
    
    def __init__(self, seeds=[42, 123, 456]):
        self.seeds = seeds
        self.results = []
        
    def simulate_noisy_training(self, seed=42, label_noise_level=0.1):
        """Simulate training with label noise to stress-test controllers"""
        print(f"  Running noisy training with seed {seed}, noise level {label_noise_level}")
        
        # Set seed for reproducibility
        random.seed(seed)
        np.random.seed(seed)
        
        # Initialize controllers
        extend_stop = ExtendStop(patience=5, min_delta=1e-4, max_extensions=2)
        gradient_monitor = GradientEntropyMonitor(window_size=3, entropy_drop_threshold=0.25)
        auto_tuner = AutoLrPrecisionTuner(initial_lr=1e-3, min_lr=1e-7)
        
        # Counters for events
        false_stops = 0
        controller_reverts = 0
        oscillation_events = 0
        
        # Simulate training with noise
        val_metrics = [0.85]  # Initial validation metric
        max_epochs = 100
        
        # Track precision changes to detect oscillation
        precision_history = []
        lr_history = []
        
        for epoch in range(max_epochs):
            # Simulate noisy validation metric
            # Add label noise that can cause temporary improvements
            noise = random.uniform(-label_noise_level, label_noise_level)
            
            # Base improvement that decreases over time
            base_improvement = max(0, 0.005 * (1.0 - epoch/50))
            
            # Actual improvement with noise
            actual_improvement = base_improvement + noise
            new_metric = min(0.99, max(0.5, val_metrics[-1] + actual_improvement))
            val_metrics.append(new_metric)
            
            # Apply controllers
            extend_action = extend_stop(new_metric)
            
            # Count false stops (stops when there's still significant potential for improvement)
            if extend_action["action"] == "stop":
                # Check if this is a false stop by looking at recent improvements
                # Only count as false stop if improvements are substantial and consistent
                recent_improvements = [val_metrics[i] - val_metrics[i-1] for i in range(max(1, len(val_metrics)-5), len(val_metrics))]
                significant_improvements = [imp for imp in recent_improvements if imp > 1e-3]
                if len(significant_improvements) >= 4:  # At least 4 significant improvements
                    false_stops += 1
                    
            # Simulate gradient monitoring
            if epoch > 10:
                # Create a mock model for gradient monitoring
                class MockModel:
                    def parameters(self):
                        # Return mock parameters with random gradients
                        class MockParam:
                            def __init__(self):
                                self.grad = None if random.random() < 0.3 else MockTensor()
                        class MockTensor:
                            def abs(self):
                                return self
                            def mean(self):
                                class MockItem:
                                    def item(self):
                                        return random.uniform(0.01, 0.1)
                                return MockItem()
                        return [MockParam() for _ in range(5)]
                
                mock_model = MockModel()
                grad_recommendations = gradient_monitor.should_adjust_training(mock_model)
                
                # Simulate tuner actions
                tuner_result = auto_tuner.adjust_lr_and_precision(None, new_metric, 
                                                                 gradient_monitor, mock_model,
                                                                 loss_improving=actual_improvement > 1e-4)
                
                # Track precision and LR changes for oscillation detection
                precision_history.append(tuner_result["precision"])
                lr_history.append(tuner_result["lr"])
                
                # Detect precision oscillations (frequent switching)
                if len(precision_history) > 3:
                    recent_precisions = precision_history[-3:]
                    if len(set(recent_precisions)) > 1:  # Multiple different precisions
                        unique_switches = sum(1 for i in range(1, len(recent_precisions)) 
                                            if recent_precisions[i] != recent_precisions[i-1])
                        if unique_switches >= 2:
                            oscillation_events += 1
                
                # Count controller reverts (actions that undo previous actions)
                if tuner_result["action"] == "increase_precision" and len(precision_history) > 1:
                    if precision_history[-2] == "fp16" and tuner_result["precision"] == "fp32":
                        controller_reverts += 1
        
        result = {
            'seed': seed,
            'false_stops': false_stops,
            'controller_reverts': controller_reverts,
            'oscillation_events': oscillation_events,
            'total_epochs': len(val_metrics) - 1,
            'final_metric': val_metrics[-1]
        }
        
        return result
    
    def run_experiment(self):
        """Run the false-stop/oscillation robustness experiment"""
        print("Running False-Stop / Oscillation Robustness Experiment")
        print("=" * 55)
        
        # Test with different noise levels
        noise_levels = [0.05, 0.1, 0.15]  # Low, medium, high noise
        
        for noise_level in noise_levels:
            print(f"\nTesting with label noise level: {noise_level}")
            level_results = []
            
            for seed in self.seeds:
                result = self.simulate_noisy_training(seed=seed, label_noise_level=noise_level)
                level_results.append(result)
                print(f"    Seed {seed}: False stops={result['false_stops']}, "
                      f"Reverts={result['controller_reverts']}, "
                      f"Oscillations={result['oscillation_events']}")
            
            # Calculate statistics for this noise level
            avg_false_stops = np.mean([r['false_stops'] for r in level_results])
            avg_reverts = np.mean([r['controller_reverts'] for r in level_results])
            avg_oscillations = np.mean([r['oscillation_events'] for r in level_results])
            
            # Calculate false stop rate as percentage of total runs
            total_runs = len(level_results) * 100  # Assuming ~100 potential stopping points per run
            false_stop_rate = (sum(r['false_stops'] for r in level_results) / total_runs) * 100
            
            print(f"    Average: False stops={avg_false_stops:.1f}, "
                  f"Reverts={avg_reverts:.1f}, "
                  f"Oscillations={avg_oscillations:.1f}")
            print(f"    False stop rate: {false_stop_rate:.2f}%")
            
            self.results.append({
                'noise_level': noise_level,
                'avg_false_stops': avg_false_stops,
                'avg_reverts': avg_reverts,
                'avg_oscillations': avg_oscillations,
                'false_stop_rate': false_stop_rate,
                'results': level_results
            })
        
        # Analyze overall results
        self._analyze_results()
        
    def _analyze_results(self):
        """Analyze experiment results"""
        print(f"\n{'='*55}")
        print("ROBUSTNESS EXPERIMENT RESULTS ANALYSIS")
        print("=" * 55)
        
        # Check if false stop rate is acceptable (< 5%)
        all_false_stop_rates = [r['false_stop_rate'] for r in self.results]
        mean_false_stop_rate = np.mean(all_false_stop_rates)
        std_false_stop_rate = np.std(all_false_stop_rates)
        
        print(f"\nFalse Stop Rate Analysis:")
        print(f"  Mean false stop rate: {mean_false_stop_rate:.2f}% ¬± {std_false_stop_rate:.2f}%")
        print(f"  Acceptance criterion: ‚â§ 5%")
        false_stop_acceptable = mean_false_stop_rate <= 5.0
        print(f"  Result: {'‚úÖ PASS' if false_stop_acceptable else '‚ùå FAIL'}")
        
        # Check oscillation robustness
        all_oscillations = [r['avg_oscillations'] for r in self.results]
        mean_oscillations = np.mean(all_oscillations)
        std_oscillations = np.std(all_oscillations)
        
        print(f"\nOscillation Robustness Analysis:")
        print(f"  Mean oscillation events: {mean_oscillations:.2f} ¬± {std_oscillations:.2f}")
        print(f"  Lower is better for robustness")
        
        # Check controller reverts
        all_reverts = [r['avg_reverts'] for r in self.results]
        mean_reverts = np.mean(all_reverts)
        std_reverts = np.std(all_reverts)
        
        print(f"\nController Revert Analysis:")
        print(f"  Mean controller reverts: {mean_reverts:.2f} ¬± {std_reverts:.2f}")
        print(f"  Lower is better for stability")
        
        # Summary
        print(f"\n{'='*55}")
        if false_stop_acceptable:
            print("üéâ FALSE-STOP/OSCILLATION ROBUSTNESS VALIDATED!")
            print(f"   ‚Ä¢ False stop rate: {mean_false_stop_rate:.2f}% (‚â§5% ‚úì)")
            print(f"   ‚Ä¢ Oscillation events: {mean_oscillations:.2f} (robust)")
            print(f"   ‚Ä¢ Controller reverts: {mean_reverts:.2f} (stable)")
        else:
            print("‚ö†Ô∏è  Robustness needs improvement")
            print(f"   ‚Ä¢ False stop rate: {mean_false_stop_rate:.2f}% (>5% ‚úó)")
        print(f"{'='*55}")

def main():
    """Main function to run the experiment"""
    print("MAHIA-V5 False-Stop / Oscillation Robustness Experiment")
    print("=" * 55)
    
    try:
        # Run experiment with 3 random seeds
        experiment = RobustnessExperiment(seeds=[42, 123, 456])
        experiment.run_experiment()
        
        print("\n‚úÖ False-stop/oscillation robustness experiment completed!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Experiment failed with error: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    main()